#!/usr/bin/env python

from datetime import datetime
from itertools import imap
import os
import glob
import predict_this.text.text as Text
import predict_this.flm.flm_specification as FlmSpec

LOG_FILENAME = "training.log"

log_file = open(LOG_FILENAME, "a")


def now():
    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


def log(s):
    s = now()+": "+ s
    print s
    print >>log_file, s
    log_file.flush()


def dump_training_file(flm_spec, output_file):
    for i in xrange(10000):
        index = str(i).zfill(4)
        tagged_filename = "corpus/tagged_chunks/%s.ascii" % index
        tagged_text = Text.Text.from_freeling_output_file(tagged_filename)
        # TODO: use tmpfiles
        with open("corpus/factored_chunks/" + index, "w") as factored_file:
            for tagged_line in tagged_text.lines():
                factored_file.write(" ".join(map(flm_spec.convert_to_flm_format, tagged_line)) + "\n")
    os.system("./concatenate_files.sh corpus/factored_chunks/ \"*\" " + output_file)




with open("train_all_models.sh", "w") as training_script:
    print >>training_script, "#!/usr/bin/env bash\n"
    print >>training_script, "# AUTOGENERATED SCRIPT at %s \n" % now()

    any_training_necessary = False
    
    for flm_model_filename in glob.glob("flm_models/*.flm"):

        log("factoring text for " + flm_model_filename)
        flm_spec = FlmSpec.FLM_Specification(flm_model_filename)

        if os.path.isfile(flm_spec.model_file()):
            log("'%s' already exists, skip training." % flm_spec.model_file())
        else:
            any_training_necessary = True
            used_factors = "".join(sorted(["W"] + list(flm_spec.factors())))
            factored_file_filename = "flm_models/%s.factored" % used_factors
            if os.path.isfile(factored_file_filename):
                log(factored_file_filename + " already exists, skip factoring")
            else:
                log("factoring " + flm_model_filename)
                dump_training_file(flm_spec, factored_file_filename)
                log("done factoring " + flm_model_filename)
            print >>training_script, "echo \"$(date): training %s\" | tee %s" % (flm_model_filename, LOG_FILENAME)
            print >>training_script, "fngram-count -factor-file %s -no-virtual-end-sentence -lm -write-counts -text %s" % \
                (flm_model_filename, factored_file_filename)

    if any_training_necessary:
        print >>training_script, "echo \"$(date): finished training\" | tee ", LOG_FILENAME
        log("done factoring and creating script for training")
        print
        print "Factored files, prepared script for training."
        print "To continue training execute ./train_all_models.sh"
    else:
        log("there is nothing to be trained")
