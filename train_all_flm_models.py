#!/usr/bin/env python

from datetime import datetime
import os
import glob
import predict_this.flm.flm_specification as FlmSpec
import sys

LOG_FILENAME = "training.log"

log_file = open(LOG_FILENAME, "a")


def now():
    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


def log(s):
    s = now() + ": " + s
    print s
    print >>log_file, s
    log_file.flush()


FACTORED_CORPUS_FILE = "corpus/factored_corpus_WGNCPL.txt"

if not os.path.isfile(FACTORED_CORPUS_FILE):
    print >>sys.stderr, "You first have to create a tagged corpus."
    print >>sys.stderr, " $ cd corpus && make tagged_corpus"
    sys.exit(1)

with open("train_all_models.sh", "w") as training_script:
    print >>training_script, "#!/usr/bin/env bash\n"
    print >>training_script, "# AUTOGENERATED SCRIPT at %s \n" % now()
    print >>training_script, "set -e\n"

    any_training_necessary = False

    for flm_model_filename in glob.glob("flm_models/*.flm"):

        flm_spec = FlmSpec.FLM_Specification(flm_model_filename)

        if os.path.isfile(flm_spec.model_file()):
            log("'%s' already exists, skip training." % flm_spec.model_file())
        else:
            any_training_necessary = True
            assert flm_spec.factors() <= {"W", "G", "N", "C", "P", "L"}, " must use common factors only.. (%s)" % flm_spec.model_file()
            print >>training_script, "echo \"$(date): training %s\" | tee -a %s" % (flm_model_filename, LOG_FILENAME)
            print >>training_script, "fngram-count -factor-file %s -no-virtual-begin-sentence -nonull -no-virtual-end-sentence -unk -lm -write-counts -no-add-end-sentence-token -text %s" % \
                (flm_model_filename, FACTORED_CORPUS_FILE)

    if any_training_necessary:
        print >>training_script, "echo \"$(date): finished training\" | tee -a ", LOG_FILENAME
        log("done factoring and creating script for training")
        print
        print "Factored files, prepared script for training."
        print "To continue training execute ./train_all_models.sh"
    else:
        log("there is nothing to be trained")
